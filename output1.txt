INFO:root:Rank 0: loading gin config from configs/ml-1m/hstu-small-batch.gin
INFO:root:cuda.matmul.allow_tf32: True
INFO:root:cudnn.allow_tf32: True
INFO:root:Training model on rank 0.
Initialize _item_emb.weight as truncated normal: torch.Size([3953, 25]) params
Skipping init for _embedding_module._item_emb.weight
Initialize _input_features_preproc._pos_emb.weight as xavier normal: torch.Size([71, 25]) params
Skipping init for _hstu._attention_layers.0._uvqk
Skipping init for _hstu._attention_layers.0._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.0._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.0._o.weight
Skipping init for _hstu._attention_layers.0._o.bias
Skipping init for _hstu._attention_layers.1._uvqk
Skipping init for _hstu._attention_layers.1._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.1._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.1._o.weight
Skipping init for _hstu._attention_layers.1._o.bias
Skipping init for _hstu._attention_layers.2._uvqk
Skipping init for _hstu._attention_layers.2._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.2._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.2._o.weight
Skipping init for _hstu._attention_layers.2._o.bias
Skipping init for _hstu._attention_layers.3._uvqk
Skipping init for _hstu._attention_layers.3._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.3._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.3._o.weight
Skipping init for _hstu._attention_layers.3._o.bias
Skipping init for _hstu._attention_layers.4._uvqk
Skipping init for _hstu._attention_layers.4._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.4._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.4._o.weight
Skipping init for _hstu._attention_layers.4._o.bias
Skipping init for _hstu._attention_layers.5._uvqk
Skipping init for _hstu._attention_layers.5._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.5._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.5._o.weight
Skipping init for _hstu._attention_layers.5._o.bias
Skipping init for _hstu._attention_layers.6._uvqk
Skipping init for _hstu._attention_layers.6._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.6._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.6._o.weight
Skipping init for _hstu._attention_layers.6._o.bias
Skipping init for _hstu._attention_layers.7._uvqk
Skipping init for _hstu._attention_layers.7._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.7._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.7._o.weight
Skipping init for _hstu._attention_layers.7._o.bias
INFO:root:Rank 0: writing logs to ./exps/ml-1m-l60/HSTU-b8-h2-dqk25-dv25-lsilud0.2-ad0.0_DotProduct_local-l2-eps1e-06_ssl-t0.05-n128-b8-lr0.001-wu0-wd0-2025-03-25
INFO:root:rank 0:  batch-stat (eval): iter 0 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.0000, MRR 0.0017 
INFO:root: rank: 0, batch-stat (train): step 0 (epoch 0 in 41.01s): 10.891322
INFO:root:rank 0:  batch-stat (eval): iter 100 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.2500, MRR 0.0149 
INFO:root: rank: 0, batch-stat (train): step 100 (epoch 0 in 10.42s): 4.508720
INFO:root:rank 0:  batch-stat (eval): iter 200 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.0000, MRR 0.0038 
INFO:root: rank: 0, batch-stat (train): step 200 (epoch 0 in 9.95s): 4.534386
INFO:root:rank 0:  batch-stat (eval): iter 300 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.0000, MRR 0.0025 
INFO:root: rank: 0, batch-stat (train): step 300 (epoch 0 in 10.81s): 4.040954
INFO:root:rank 0:  batch-stat (eval): iter 400 (epoch 0): NDCG@10 0.1250, HR@10 0.1250, HR@50 0.1250, MRR 0.1284 
INFO:root: rank: 0, batch-stat (train): step 400 (epoch 0 in 10.17s): 4.330012
INFO:root:rank 0:  batch-stat (eval): iter 500 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.0000, MRR 0.0036 
INFO:root: rank: 0, batch-stat (train): step 500 (epoch 0 in 9.87s): 3.810736
INFO:root:rank 0:  batch-stat (eval): iter 600 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.1250, MRR 0.0128 
INFO:root: rank: 0, batch-stat (train): step 600 (epoch 0 in 9.99s): 3.778574
INFO:root:rank 0:  batch-stat (eval): iter 700 (epoch 0): NDCG@10 0.2039, HR@10 0.2500, HR@50 0.3750, MRR 0.1957 
INFO:root: rank: 0, batch-stat (train): step 700 (epoch 0 in 9.87s): 3.357761
INFO:root:rank 0: eval @ epoch 0 in 55.99s: NDCG@10 0.0495, NDCG@50 0.0834, HR@10 0.0962, HR@50 0.2535, MRR 0.0458
INFO:root:rank 0:  batch-stat (eval): iter 800 (epoch 1): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.2500, MRR 0.0122 
INFO:root: rank: 0, batch-stat (train): step 800 (epoch 1 in 46.84s): 3.675815
INFO:root:rank 0:  batch-stat (eval): iter 900 (epoch 1): NDCG@10 0.2894, HR@10 0.3750, HR@50 0.5000, MRR 0.2750 
INFO:root: rank: 0, batch-stat (train): step 900 (epoch 1 in 10.02s): 3.210908
INFO:root:rank 0:  batch-stat (eval): iter 1000 (epoch 1): NDCG@10 0.0376, HR@10 0.1250, HR@50 0.2500, MRR 0.0229 
INFO:root: rank: 0, batch-stat (train): step 1000 (epoch 1 in 9.76s): 3.504256
INFO:root:rank 0:  batch-stat (eval): iter 1100 (epoch 1): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.6250, MRR 0.0220 
INFO:root: rank: 0, batch-stat (train): step 1100 (epoch 1 in 10.31s): 2.590305
INFO:root:rank 0:  batch-stat (eval): iter 1200 (epoch 1): NDCG@10 0.0417, HR@10 0.1250, HR@50 0.1250, MRR 0.0231 
INFO:root: rank: 0, batch-stat (train): step 1200 (epoch 1 in 10.46s): 2.979336
INFO:root:rank 0:  batch-stat (eval): iter 1300 (epoch 1): NDCG@10 0.0625, HR@10 0.1250, HR@50 0.1250, MRR 0.0440 
INFO:root: rank: 0, batch-stat (train): step 1300 (epoch 1 in 10.41s): 3.961474
INFO:root:rank 0:  batch-stat (eval): iter 1400 (epoch 1): NDCG@10 0.2500, HR@10 0.2500, HR@50 0.2500, MRR 0.2559 
INFO:root: rank: 0, batch-stat (train): step 1400 (epoch 1 in 10.08s): 3.338499
INFO:root:rank 0:  batch-stat (eval): iter 1500 (epoch 1): NDCG@10 0.0484, HR@10 0.1250, HR@50 0.3750, MRR 0.0373 
INFO:root: rank: 0, batch-stat (train): step 1500 (epoch 1 in 10.00s): 2.989609
INFO:root:rank 0: eval @ epoch 1 in 46.76s: NDCG@10 0.0831, NDCG@50 0.1270, HR@10 0.1538, HR@50 0.3565, MRR 0.0740
