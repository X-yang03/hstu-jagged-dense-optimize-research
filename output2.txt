INFO:root:Rank 0: loading gin config from configs/ml-1m/hstu-small-batch.gin
INFO:root:cuda.matmul.allow_tf32: True
INFO:root:cudnn.allow_tf32: True
INFO:root:Training model on rank 0.
Initialize _item_emb.weight as truncated normal: torch.Size([3953, 25]) params
Skipping init for _embedding_module._item_emb.weight
Initialize _input_features_preproc._pos_emb.weight as xavier normal: torch.Size([71, 25]) params
Skipping init for _hstu._attention_layers.0._uvqk
Skipping init for _hstu._attention_layers.0._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.0._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.0._o.weight
Skipping init for _hstu._attention_layers.0._o.bias
Skipping init for _hstu._attention_layers.1._uvqk
Skipping init for _hstu._attention_layers.1._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.1._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.1._o.weight
Skipping init for _hstu._attention_layers.1._o.bias
Skipping init for _hstu._attention_layers.2._uvqk
Skipping init for _hstu._attention_layers.2._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.2._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.2._o.weight
Skipping init for _hstu._attention_layers.2._o.bias
Skipping init for _hstu._attention_layers.3._uvqk
Skipping init for _hstu._attention_layers.3._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.3._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.3._o.weight
Skipping init for _hstu._attention_layers.3._o.bias
Skipping init for _hstu._attention_layers.4._uvqk
Skipping init for _hstu._attention_layers.4._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.4._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.4._o.weight
Skipping init for _hstu._attention_layers.4._o.bias
Skipping init for _hstu._attention_layers.5._uvqk
Skipping init for _hstu._attention_layers.5._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.5._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.5._o.weight
Skipping init for _hstu._attention_layers.5._o.bias
Skipping init for _hstu._attention_layers.6._uvqk
Skipping init for _hstu._attention_layers.6._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.6._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.6._o.weight
Skipping init for _hstu._attention_layers.6._o.bias
Skipping init for _hstu._attention_layers.7._uvqk
Skipping init for _hstu._attention_layers.7._rel_attn_bias._ts_w
Skipping init for _hstu._attention_layers.7._rel_attn_bias._pos_w
Skipping init for _hstu._attention_layers.7._o.weight
Skipping init for _hstu._attention_layers.7._o.bias
INFO:root:Rank 0: writing logs to ./exps/ml-1m-l60/HSTU-b8-h2-dqk25-dv25-lsilud0.2-ad0.0_DotProduct_local-l2-eps1e-06_ssl-t0.05-n128-b8-lr0.001-wu0-wd0-2025-03-25
INFO:root:rank 0:  batch-stat (eval): iter 0 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.0000, MRR 0.0019 
INFO:root: rank: 0, batch-stat (train): step 0 (epoch 0 in 41.69s): 10.360463
INFO:root:rank 0:  batch-stat (eval): iter 100 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.2500, MRR 0.0204 
INFO:root: rank: 0, batch-stat (train): step 100 (epoch 0 in 24.27s): 4.466816
INFO:root:rank 0:  batch-stat (eval): iter 200 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.1250, MRR 0.0060 
INFO:root: rank: 0, batch-stat (train): step 200 (epoch 0 in 24.63s): 4.512199
INFO:root:rank 0:  batch-stat (eval): iter 300 (epoch 0): NDCG@10 0.0000, HR@10 0.0000, HR@50 0.0000, MRR 0.0027 
INFO:root: rank: 0, batch-stat (train): step 300 (epoch 0 in 26.33s): 4.028134
INFO:root:rank 0:  batch-stat (eval): iter 400 (epoch 0): NDCG@10 0.1250, HR@10 0.1250, HR@50 0.1250, MRR 0.1279 
INFO:root: rank: 0, batch-stat (train): step 400 (epoch 0 in 25.00s): 4.261068
INFO:root:rank 0:  batch-stat (eval): iter 500 (epoch 0): NDCG@10 0.0361, HR@10 0.1250, HR@50 0.2500, MRR 0.0178 
INFO:root: rank: 0, batch-stat (train): step 500 (epoch 0 in 24.40s): 3.573793
INFO:root:rank 0:  batch-stat (eval): iter 600 (epoch 0): NDCG@10 0.0756, HR@10 0.2500, HR@50 0.3750, MRR 0.0356 
INFO:root: rank: 0, batch-stat (train): step 600 (epoch 0 in 24.06s): 3.635182
INFO:root:rank 0:  batch-stat (eval): iter 700 (epoch 0): NDCG@10 0.1667, HR@10 0.2500, HR@50 0.5000, MRR 0.1619 
INFO:root: rank: 0, batch-stat (train): step 700 (epoch 0 in 23.82s): 3.073789
INFO:root:rank 0: eval @ epoch 0 in 49.37s: NDCG@10 0.0704, NDCG@50 0.1127, HR@10 0.1346, HR@50 0.3286, MRR 0.0631
